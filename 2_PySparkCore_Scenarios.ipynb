{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a706741d-1790-46f5-aff3-b65ed48b9f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###PySpark Core Use Cases\n",
    "\n",
    "After trying/completing the below interesting simple usecases, you will be definately getting some handson and improving programming skills in PySpark Core.\n",
    "\n",
    "#Note\n",
    " \n",
    "- Import this notebook into the databricks environment\n",
    "- Create the cells below to write %pyspark code under every markups given below\n",
    "- If you are not able to achieve the result as I expected, try to get it as per the way you prefer.\n",
    "- If you are failing for 1st time, try several times to make it succeeded, \n",
    "- Complete the other scenarios and come back if you feel struck in some use cases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5aecca-f3cf-4a80-9155-0f7a8e6f9cda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 1. Enable the DBFS File Browser (Top right Login -> Settings -> Advanced -> DBFS File Browser (Enable or disable DBFS File Browser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d0473cd-c6d8-410b-907a-398aea1c44e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 2. Upload the given file youtube_videos.tsv into the dbfs location by browsing (catalog -> browse DBFS -> /FileStore/datafiles/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "866d7866-178f-4dd2-bd2c-ce0bf59c27de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e12b410f-1118-4e98-b9db-9edd222fbb0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3. Create an RDD dbfs_rdd to read the above data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d446966-0c56-409a-9350-853332d12173",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 4. Split the rows using tab (\"\\t\") delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a30384-f3bf-4df2-965c-907c2c115a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 5. Remove the header record by filtering the first column value does not contains \"id\" into an rdd split_rdd or try using take/first/zipWithIndex function to remove the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "246c10f4-0280-4d3b-9c83-1425130029d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 6. cache the split_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e03dcad6-2501-4779-b348-27a083718445",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######7. Display only first 10 rows in the screen from spli_trdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a391a69-1846-4c0c-a718-6db4035d431c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######8. Filter only Music category data from split_rdd into an rdd called music_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6712e6-073c-4a99-89e9-9c122ea1042d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######9. Filter only duration>100 data from split_rdd into an rdd called long_dur_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73981523-64e7-4211-be05-bff3bf4caefc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######10. Union music_rdd with longdur_rdd then convert to tuple and get only the deduplicated (distinct) records into an rdd music_longdur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b4777c-fb11-492d-ae31-204af7e12995",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######11. Select only id, duration, codec and category by re-ordering the fields like id,category,codec,duration into an rdd map_colsrdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a783a385-b7b7-4f7b-b80b-4b385740c4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######12. Select only duration column from map_colsrdd and find max duration by using max function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e60227f-cf0a-4d4c-88f3-5bcae5b8f943",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######13. Select only codec from map_colsrdd, convert to upper case and print distinct of it in the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e272f50-6e8c-45e0-a4b0-34e93c460d47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######14. Create an rdd called filerdd_4part from file_rdd created in step1 by increasing the number of partitions to 4 (Execute this step anywhere in the code where ever appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf316502-68ec-47c8-89e9-57064d049634",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######15. Persist the filerdd4part data into memory and disk with replica of 2, (Execute this step anywhere in the code where ever appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1447833a-59a1-4b1c-8e9b-493398b979d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######16. Calculate and print the overall total, max, min duration for Comedy category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29304993-e1e9-495d-9af9-61fd2359c724",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######17. Print the codec wise count and minimum duration not by using min rather try to use reduce function function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c912adc1-2046-4d41-b68b-a785f1ae38cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######18. Print the distinct category of videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5109c91f-b79d-424f-8f82-ec3b3d334811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######19. Print only the id, duration, height, category and width sorted by duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0ee1f8-43fa-474a-ad80-cc472341d276",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######20. Create a python function called masking which should take the string as input and returns the hash value of the input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f2c51a6-a002-49eb-9ae1-5419c65d15ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######21. Call the masking function created in the above step and pass category column and get the hashed value of category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6051fea1-8ca6-41d1-813c-42733aa65e74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######22. Store the step 18 result in a dbfs location in a single file with data delimited as | with the id, duration, height, masking(category) and width columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a18ed28c-0941-43de-bf0d-fd1544826141",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######23. Try to implement few performance optimization factors like partitioning, caching and broadcasting (whereever applicable)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_PySparkCore_Scenarios",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
